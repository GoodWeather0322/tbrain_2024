{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 21822.60it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ckip_ws(text):\n",
    "    wss = ws_driver(text)\n",
    "    result = []\n",
    "    for ws in wss:\n",
    "        result.append(' '.join(ws))\n",
    "\n",
    "    return result\n",
    "\n",
    "texts = [\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "]\n",
    "res = get_ckip_ws(texts)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "from opencc import OpenCC\n",
    "\n",
    "count = 0\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text').iterdir():\n",
    "    if folder.is_dir():\n",
    "        for file in folder.iterdir():\n",
    "            count += 1\n",
    "pbar = tqdm_notebook(total=count)\n",
    "opencc = OpenCC(\"s2t\")\n",
    "\n",
    "output_path = Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text_ckip_converted')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text').iterdir():\n",
    "    if folder.is_dir() and folder.name == \"faq\":\n",
    "        output_folder = output_path / folder.name\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        for file in folder.iterdir():\n",
    "            output_file = output_folder / file.name\n",
    "            with open(file, 'r') as f, open(output_file, 'w') as f_out:\n",
    "                text = f.read()\n",
    "                converted_text = opencc.convert(text)\n",
    "                converted_text = converted_text.replace(\" \", \"\")\n",
    "                converted_text = converted_text.split('\\n')\n",
    "                converted_text = get_ckip_ws(converted_text)\n",
    "                converted_text = '\\n'.join(converted_text)\n",
    "                f_out.write(converted_text)\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3146.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16070.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17476.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19878.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21620.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24966.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17476.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16710.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17924.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15196.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7049.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16194.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16644.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16980.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17331.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17772.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16644.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18157.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16980.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17623.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16644.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17623.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14768.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17623.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16980.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 25115.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23172.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19878.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20068.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18236.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14665.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16710.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21845.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17476.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23172.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 25731.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24528.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 26214.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/dataset/preliminary/questions_example.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "questions = data['questions']\n",
    "\n",
    "for question in questions:\n",
    "    question['query'] = get_ckip_ws([question['query']])[0]\n",
    "\n",
    "with open('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/dataset/preliminary/questions_example_ckip.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docxchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
