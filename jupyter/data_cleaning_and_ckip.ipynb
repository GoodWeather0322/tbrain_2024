{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是範例 測試 測試 page 其他內容 結尾\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub(r'\\*\\*page \\d+\\*\\*', '', text)\n",
    "    text = re.sub(r'\\*\\*question \\d+\\*\\*', '', text)\n",
    "    text = re.sub(r'\\*\\*answer \\d+\\*\\*', '', text)\n",
    "    # Step 1: 去除網址和 EMAIL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\n",
    "\n",
    "    text = re.sub(r'【[A-Za-z0-9]+】', '', text)\n",
    "\n",
    "    # Step 6: 去除 \"第 X 頁，共 Y 頁\" 格式\n",
    "    text = re.sub(r'第 \\d+ 頁，共 \\d+ 頁', '', text)\n",
    "\n",
    "    # Step 7: 去除 \"X/Y\" 或 \"X / Y\" 格式\n",
    "    text = re.sub(r'\\b\\d+ ?/ ?\\d+\\b', '', text)\n",
    "\n",
    "    # Step 8: 去除 \"~X~\" 格式\n",
    "    text = re.sub(r'~\\d+~', '', text)\n",
    "\n",
    "    # Step 9: 去除 \"（接次頁）\" 和 \"（承前頁）\"\n",
    "    text = re.sub(r'（接次頁）|（承前頁）', '', text)\n",
    "\n",
    "    # Step 10: 去除 \"- X -\" 格式\n",
    "    text = re.sub(r'- \\d+ -', '', text)\n",
    "\n",
    "    # Step 2: 去除無意義數字（可以依需求調整，如果想保留某些數字格式）\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "    # Step 3: 去除標點符號\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 去除多餘的空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# 測試範例\n",
    "sample_text = \"\"\"\n",
    "這是範例 https://example.com 測試 email@example.com 12345 測試！ **page 1 ** 其他內容 【APMOPL】\n",
    "第 11 頁，共 24 頁\n",
    "第 12 頁，共 24 頁\n",
    "3/14\n",
    "5/22\n",
    "12 / 17\n",
    "【APKQTL】-11/16\n",
    "~7~\n",
    "（接次頁）\n",
    "（承前頁）\n",
    "- 15 -\n",
    "結尾。\n",
    "\"\"\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text').iterdir():\n",
    "    if folder.is_dir():\n",
    "        for file in folder.iterdir():\n",
    "            count += 1\n",
    "pbar = tqdm_notebook(total=count)\n",
    "opencc = OpenCC(\"s2t\")\n",
    "\n",
    "output_path = Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text_cleaned')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text').iterdir():\n",
    "    if folder.is_dir():\n",
    "        output_folder = output_path / folder.name\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        for file in folder.iterdir():\n",
    "            output_file = output_folder / file.name\n",
    "            with open(file, 'r') as f, open(output_file, 'w') as f_out:\n",
    "                text = f.read()\n",
    "                text = opencc.convert(text)\n",
    "                texts = text.split('\\n')\n",
    "                for text in texts:\n",
    "                    text = clean_text(text)\n",
    "                    f_out.write(text + '\\n')\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 18657.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會', '美國 參議院 針對 今天 總統 布什 所 提名 的 勞工部長 趙小蘭 展開 認可 聽證會']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ckip_ws(text):\n",
    "    wss = ws_driver(text)\n",
    "    result = []\n",
    "    for ws in wss:\n",
    "        result.append(' '.join(ws))\n",
    "\n",
    "    return result\n",
    "\n",
    "texts = [\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "    \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會\",\n",
    "]\n",
    "res = get_ckip_ws(texts)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "from opencc import OpenCC\n",
    "\n",
    "count = 0\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text_cleaned').iterdir():\n",
    "    if folder.is_dir():\n",
    "        for file in folder.iterdir():\n",
    "            count += 1\n",
    "pbar = tqdm_notebook(total=count)\n",
    "opencc = OpenCC(\"s2t\")\n",
    "\n",
    "output_path = Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text_cleaned_ckip_converted')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for folder in Path('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/reference_text_cleaned').iterdir():\n",
    "    if folder.is_dir() and folder.name == \"finance\":\n",
    "        output_folder = output_path / folder.name\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        for file in folder.iterdir():\n",
    "            output_file = output_folder / file.name\n",
    "            with open(file, 'r') as f, open(output_file, 'w') as f_out:\n",
    "                text = f.read()\n",
    "                if text.strip() == \"\":\n",
    "                    text = 'empty'\n",
    "                converted_text = opencc.convert(text)\n",
    "                converted_text = converted_text.replace(\" \", \"\")\n",
    "                converted_text = converted_text.split('\\n')\n",
    "                converted_text = get_ckip_ws(converted_text)\n",
    "                converted_text = '\\n'.join(converted_text)\n",
    "                f_out.write(converted_text)\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8192.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17623.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18157.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19691.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 26214.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 25575.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20068.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15650.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22429.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16980.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23172.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22310.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17924.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15141.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18236.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17924.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16320.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19878.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14768.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21845.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17924.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15420.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16448.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16448.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18315.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15887.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18236.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24244.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17848.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8388.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1663.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18157.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18157.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22795.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17772.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21620.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 26886.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 22310.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20068.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/dataset/preliminary/questions_example.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "questions = data['questions']\n",
    "\n",
    "for question in questions:\n",
    "    question['query'] = clean_text(question['query'])\n",
    "    question['query'] = get_ckip_ws([question['query']])[0]\n",
    "\n",
    "with open('/Users/goodweather/Desktop/workspace/tbrain_2024/source/競賽資料集/dataset/preliminary/questions_example_cleaned_ckip.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docxchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
